{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-upgrade",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import cluster\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import feature_selection as fs\n",
    "from sklearn import preprocessing as prepro\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from numpy import mean\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-locking",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "engine = create_engine(\"postgres://postgres:postgres@localhost/ecommerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-stanley",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class EstimatorSelectionHelper:\n",
    "\n",
    "    def __init__(self, models, params, scoring=['accuracy'], cv=5):\n",
    "        if not set(models.keys()).issubset(set(params.keys())):\n",
    "            missing_params = list(set(models.keys()) - set(params.keys()))\n",
    "            raise ValueError(\"Some estimators are missing parameters: %s\" % missing_params)\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "        self.scoring = scoring\n",
    "        self.cv = cv\n",
    "\n",
    "    def fit(self, X, y, n_jobs=3, verbose=1, refit=False, cv=5):\n",
    "        for key in self.keys:\n",
    "            print(\"Running GridSearchCV for %s.\" % key)\n",
    "            model = self.models[key]\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=self.cv, n_jobs=n_jobs,\n",
    "                              verbose=verbose, scoring=self.scoring, refit=refit,\n",
    "                              return_train_score=True)\n",
    "            gs.fit(X,y)\n",
    "            self.grid_searches[key] = gs\n",
    "\n",
    "    def score_summary(self):\n",
    "        def row(key, scores, params):\n",
    "            d = {}\n",
    "            d['estimator'] = key\n",
    "            for score, values in scores.items():\n",
    "                d['min_score_{}'.format(score)] = min(values)\n",
    "                d['max_score_{}'.format(score)] = max(values)\n",
    "                d['mean_score_{}'.format(score)] = np.mean(values)\n",
    "                d['std_score_{}'.format(score)] = np.std(values)\n",
    "\n",
    "            return pd.Series({**params, **d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores_list = {}\n",
    "            for p_i, val in enumerate(params, start=0):\n",
    "                #print(\"PARAMS {} at index {}\".format(str(val), p_i))\n",
    "                scores_list[str(val)] = {}\n",
    "                for score in self.scoring:\n",
    "                    #print(\"SCORE: {}\".format(score))\n",
    "                    scores_list[str(val)][score] = []\n",
    "                    for i in range(0, self.cv):\n",
    "                        key = \"split{}_test_{}\".format(i, score)\n",
    "                        #print(\"KEY {}\".format(key))\n",
    "                        r = self.grid_searches[k].cv_results_[key]\n",
    "                        #print(\"VAL {} at index {}\".format(r, p_i))\n",
    "                        scores_list[str(val)][score].append(r[p_i])\n",
    "\n",
    "            for param in params:\n",
    "                rows.append((row(k, scores_list[str(param)], param)))\n",
    "        df = pd.DataFrame(rows)\n",
    "\n",
    "        score_cols = []\n",
    "        for score in self.scoring:\n",
    "            score_cols += ['min_score_{}'.format(score), 'mean_score_{}'.format(score), 'max_score_{}'.format(score), 'std_score_{}'.format(score)]\n",
    "        columns = ['estimator'] + score_cols\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "\n",
    "        return df[columns]\n",
    "\n",
    "    @staticmethod\n",
    "    def search_plot(df, score, x, y, ax=None):\n",
    "        # TODO:  An automatic search given the whole score_summary should be implemented.\n",
    "        df = df.pivot(index=y, columns=x, values=score)\n",
    "        return sns.heatmap(df, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-arcade",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class PCAw(PCA):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        PCA.__init__(self, *args, **kwargs)\n",
    "\n",
    "    def variance_plot(self, ax=None):\n",
    "        \"\"\"\n",
    "        Method that given a PCA instance\n",
    "        returns a plot with the relative\n",
    "        and accumulative variance.\n",
    "        \"\"\"\n",
    "        pc_variance_df = pd.DataFrame({\n",
    "            'accumulative_variance': np.cumsum(self.explained_variance_ratio_),\n",
    "            'relative_variance': self.explained_variance_ratio_\n",
    "            })\n",
    "\n",
    "        plot = pc_variance_df.plot(kind='bar', ax=ax)\n",
    "        plot.axhline(y=0.95, color='r', linestyle='--')\n",
    "        plot.set_title(\"PCA Explained variance\")\n",
    "        plot.set_xlabel(\"Principal Components\")\n",
    "        plot.set_ylabel(\"Variance\")\n",
    "\n",
    "        return plot\n",
    "\n",
    "    def plot_contribution(self, index, columns, ax=None):\n",
    "        eigenvalues=self.components_\n",
    "        pc=abs(eigenvalues[index,:])\n",
    "        contributions = pd.DataFrame({'contribution': pc}, index=columns)\n",
    "        ax = contributions.sort_values(by='contribution', ascending=False).plot(kind='bar', title=\"Contribution of variables to DIM {}\".format(index), ax=ax)\n",
    "        ax.axhline(y=0.1, color='r', linestyle='--')\n",
    "        return ax\n",
    "\n",
    "def plot_silhouette_method(df, k_min=2, k_max=10, ax=None):\n",
    "    silhouette = []\n",
    "    for k in range(k_min, k_max):\n",
    "        kmeans = cluster.KMeans(n_clusters=k)\n",
    "        clusters = kmeans.fit_predict(df)\n",
    "        silhouette.append(silhouette_score(df, clusters))\n",
    "    if ax is not None:\n",
    "        return ax.plot(range(k_min,k_max), silhouette, marker='o')\n",
    "    else:\n",
    "        return plt.plot(range(k_min,k_max), silhouette, marker='o')\n",
    "\n",
    "def plot_elbow_method(df, k_min=1, k_max=10, ax=None):\n",
    "    sse = []\n",
    "    # Apliquem KMeans pel rang de k especificat\n",
    "    for k in range(k_min, k_max):\n",
    "        kmeans = cluster.KMeans(n_clusters=k)\n",
    "        # Afegim les dades\n",
    "        kmeans.fit(df)\n",
    "        # Obtenim SSE\n",
    "        sse.append(kmeans.inertia_)\n",
    "\n",
    "    if ax is not None:\n",
    "        return ax.plot(range(k_min, k_max), sse, marker='o')\n",
    "    else:\n",
    "        return plt.plot(range(k_min, k_max), sse, marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-passage",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class EcommerceWraper:\n",
    "    numeric_cols = [\"ipcasos\", \"total_consumos\", \"visit_days\", \"vists_per_day\", \"fichas_basicas\", \"perfil_promocional\", \"same_section\", \"same_group\", \"same_class\"]\n",
    "    \n",
    "    def __init__(self, df_usuarios):\n",
    "        self.df_usuarios = df_usuarios\n",
    "    \n",
    "    def get_target_variable(self,):\n",
    "        return self.df_usuarios.ind_cliente.values\n",
    "    \n",
    "    def train_test_split(self, df, test_size=0.3):\n",
    "        \n",
    "        #true_labels = df.loc[self.df_usuarios.ind_cliente == 1, :]\n",
    "        #true_target = self.df_usuarios.loc[self.df_usuarios.ind_cliente == 1, [\"ind_cliente\"]]\n",
    "        #false_labels = df.loc[self.df_usuarios.ind_cliente == 0, :]\n",
    "        #false_target = self.df_usuarios.loc[self.df_usuarios.ind_cliente == 0, [\"ind_cliente\"]]\n",
    "        \n",
    "        #X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(true_labels, true_target, test_size=test_size)\n",
    "        #X_train_f, X_test_f, y_train_f, y_test_f = train_test_split(false_labels, false_targets, test_size=test_size)\n",
    "        target = self.get_target_variable()\n",
    "        return train_test_split(df, target, test_size=test_size, stratify=target)\n",
    "    \n",
    "    def _get_dummies(self, col_name):\n",
    "        return pd.get_dummies(self.df_usuarios[[col_name]].astype(str), prefix=col_name)\n",
    "    \n",
    "    def _get_scaled(self, col_name):\n",
    "        scaled = prepro.MinMaxScaler().fit_transform(self.df_usuarios[col_name].values.reshape(-1, 1))\n",
    "        return pd.DataFrame(scaled, columns=[col_name])\n",
    "    \n",
    "    def dummy_encode(self, scale_numeric_cols=True):\n",
    "        if scale_numeric_cols:\n",
    "            test_cols = [\"ipcasos\", \"total_consumos\", \"visit_days\", \"fichas_basicas\", \"perfil_promocional\", \"same_section\", \"same_division\", \"same_group\", \"same_class\"]\n",
    "            self.df_usuarios[test_cols] = self.df_usuarios[test_cols].clip(upper=self.df_usuarios[test_cols].quantile(0.95), axis=1)\n",
    "            \n",
    "            ipcasos = self._get_scaled(\"ipcasos\")\n",
    "            total_consumos = self._get_scaled(\"total_consumos\")\n",
    "            visit_days = self._get_scaled(\"visit_days\")\n",
    "            visits_per_day = self._get_scaled(\"visits_per_day\")\n",
    "            fichas_basicas = self._get_scaled(\"fichas_basicas\")\n",
    "            perfil_promocional = self._get_scaled(\"perfil_promocional\")\n",
    "            same_section = self._get_scaled(\"same_section\")\n",
    "            same_division = self._get_scaled(\"same_division\")\n",
    "            same_group = self._get_scaled(\"same_group\")\n",
    "            same_class = self._get_scaled(\"same_class\")\n",
    "            #print(len(same_class.index))\n",
    "        else:\n",
    "            ipcasos = self.df_usuarios[\"ipcasos\"]\n",
    "            total_consumos = self.df_usuarios[\"total_consumos\"]\n",
    "            visit_days = self.df_usuarios[\"visit_days\"]\n",
    "            visits_per_day = self.df_usuarios[\"visits_per_day\"]\n",
    "            fichas_basicas = self.df_usuarios[\"fichas_basicas\"]\n",
    "            perfil_promocional = self.df_usuarios[\"perfil_promocional\"]\n",
    "            same_section = self.df_usuarios[\"same_section\"]\n",
    "            same_division = self.df_usuarios[\"same_division\"]\n",
    "            same_group = self.df_usuarios[\"same_group\"]\n",
    "            same_class = self.df_usuarios[\"same_class\"]\n",
    "        tipousuario = self._get_dummies(\"tipousuario\").astype(int)\n",
    "        canal_registro = self._get_dummies(\"canal_registro\").astype(int)\n",
    "        #ind_cliente = self.df_usuarios[\"ind_cliente\"]\n",
    "        #ind_alta = self.df_usuarios[\"ind_alta\"].astype(int)\n",
    "        #print(len(ind_alta.index))\n",
    "        #print(\"--\")\n",
    "        tipoemail = self._get_dummies(\"tipoemail\").astype(int)\n",
    "        # We will interpret this variable as ordinal categorical.\n",
    "        # TODO: Maybe normalize?\n",
    "        bonad_email = self._get_dummies(\"bonad_email\").astype(int)\n",
    "        \n",
    "        ip_country = self._get_dummies(\"ip_country\").astype(int)\n",
    "        #print(len(ip_country.index))\n",
    "        ip_region = self._get_dummies(\"ip_region\").astype(int)\n",
    "        usu_tipo = self._get_dummies(\"usu_tipo\").astype(int)\n",
    "        usu_tamanio = self._get_dummies(\"usu_tamanio\").astype(int)\n",
    "        usu_estado = self._get_dummies(\"usu_estado\").astype(int)\n",
    "        usu_departamento = self._get_dummies(\"usu_departamento\").astype(int)\n",
    "        weekday_registro = self._get_dummies(\"weekday_registro\").astype(int)\n",
    "        workday_registro = self.df_usuarios.apply(lambda x: 1 if x[\"workday_registro\"] is True else 0, axis=1).astype(int)\n",
    "        #print(len(workday_registro.index))\n",
    "        # weekday_alta, workday_alta, weekday_cliente, workday_cliente?\n",
    "\n",
    "        phone_zone = self._get_dummies(\"phone_zone\").astype(int)\n",
    "        phone_type = self._get_dummies(\"phone_type\").astype(int)\n",
    "        region = self._get_dummies(\"region\").astype(int)\n",
    "        sub_region = self._get_dummies(\"sub-region\").astype(int)\n",
    "        intermediate_region = self._get_dummies(\"intermediate-region\").astype(int)\n",
    "        section = self._get_dummies(\"section\").astype(int)\n",
    "        division = self._get_dummies(\"division\").astype(int)\n",
    "        group = self._get_dummies(\"group\").astype(int)\n",
    "        _class = self._get_dummies(\"class\").astype(int)\n",
    "        \n",
    "        df =  pd.concat([tipousuario, canal_registro, tipoemail, bonad_email, \n",
    "                ipcasos, ip_country, ip_region, usu_tipo, usu_tamanio, usu_estado, \n",
    "                usu_departamento, weekday_registro, workday_registro, phone_zone, \n",
    "                phone_type, region, sub_region, intermediate_region, section, division, group, \n",
    "                _class, total_consumos, visit_days, visits_per_day, fichas_basicas, \n",
    "                perfil_promocional, same_section, same_division, same_group, same_class], axis=1)\n",
    "        #print(len(df.index))\n",
    "        return df\n",
    "    \n",
    "    def test_importance(self, df, target):\n",
    "        p_value = fs.chi2(df, target)[1]\n",
    "        return pd.DataFrame({\"cols\": df.columns, \"p_value\": p_value})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-detective",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-comparative",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_usuarios = pd.read_sql(\"\"\"\n",
    "SELECT *\n",
    "FROM usuarios_extra_features\n",
    "WHERE total_consumos IS NOT NULL\n",
    "\"\"\", con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-acting",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "len(df_usuarios.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-terminal",
   "metadata": {},
   "source": [
    "# Prepare data for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-lexington",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "ew = EcommerceWraper(df_usuarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-detective",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "dummy = ew.dummy_encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-mercy",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "p_value = fs.chi2(dummy, ew.get_target_variable())[1]\n",
    "desired = pd.DataFrame({\"cols\": dummy.columns, \"p_value\": p_value}).sort_values(by='p_value').query(\"p_value < 0.05\")\n",
    "desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-suite",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "dummy_selected = dummy[desired.cols.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-efficiency",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = ew.train_test_split(dummy_selected, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-glenn",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "models_svm = {\n",
    "    'SVC': SVC()\n",
    "}\n",
    "\n",
    "params_svm = {\n",
    "    'SVC': [\n",
    "        {'kernel': ['linear'], 'C': [0.01, 0.1, 1, 10, 50, 100]},\n",
    "        {'kernel': ['rbf'], 'C': [0.01, 0.1, 1, 10, 50, 100], 'gamma': [0.001, 0.0001, 0.01, 0.1]},\n",
    "        {'kernel': ['sigmoid'], 'C': [0.01, 0.1, 1, 10, 50, 100], 'gamma': [0.001, 0.0001, 0.01, 0.1]}\n",
    "    ]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-christopher",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "helper1 = EstimatorSelectionHelper(models_svm, params_svm, scoring=[\"accuracy\", \"f1\", \"recall\"])\n",
    "helper1.fit(X_train, y_train, n_jobs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-beginning",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "scores = helper1.score_summary()\n",
    "scores.sort_values(by=\"mean_score_accuracy\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-architect",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pcaw = PCAw()\n",
    "dummy_pca = pcaw.fit_transform(dummy[desired.cols.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-april",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(40, 5))\n",
    "pcaw.variance_plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-bradley",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "sns.scatterplot(x=dummy_pca[:,2], y=dummy_pca[:, 3], hue=ew.get_target_variable(), ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-creativity",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(df_usuarios[[\"tipousuario\"]].astype(str), prefix=\"tipousuario\").isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-ballet",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "scaled = prepro.StandardScaler().fit_transform(df_usuarios[\"ipcasos\"].values.reshape(-1, 1))\n",
    "pd.DataFrame(scaled, columns=[\"ipcasos\"]).isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-hygiene",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (prediccion_ecommerce)",
   "language": "python",
   "name": "pycharm-24598892"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}